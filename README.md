# ğŸ–ï¸ Hand Motion Interpretation Pipeline

### From Gesture Recognition to Sign Language Infrastructure

[![Status](https://img.shields.io/badge/status-active-success.svg)]()
[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Focus](https://img.shields.io/badge/focus-accessibility%20tech-purple.svg)]()

> **A real-time motion capture and analysis system designed as foundational infrastructure for sign language translation and accessibility technology.**

Originally built for gesture-based cursor control, this project evolved into a full motion interpretation pipeline that treats hand gestures as **structured linguistic data**, making it suitable for sign language research, animation systems, and accessibility applications.

---

## ğŸ“‘ Table of Contents

- [Project Vision](#-project-vision)
- [Technical Architecture](#ï¸-technical-architecture)
- [Key Features](#-key-features)
- [Why This Matters for Sign Language](#-why-this-matters-for-sign-language)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Dataset Overview](#-dataset-overview)
- [Key Learnings](#-key-learnings)
- [Roadmap](#-roadmap)
- [Contributing](#-contributing)
- [Author](#-author)

---

## ğŸ¯ Project Vision

Sign language translation isn't about generating smooth animations â€” it's about **preserving meaning through motion**.

This project is built on four principles:

- **Motion as data**, not just visuals
- **Sequences over frames** (signs are temporal)
- **Semantic accuracy over visual similarity**
- **Reusable pipeline architecture** (capture â†’ structure â†’ multiple outputs)

**Key insight:** The same motion capture system can power animation, training datasets, validation tools, and real-time translation â€” if the architecture treats motion as structured, reusable data.

---

## ğŸ—ï¸ Technical Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          MOTION CAPTURE LAYER              â”‚
â”‚  MediaPipe Hands + OpenCV Processing       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ABSTRACTION LAYER (CORE)            â”‚
â”‚  MotionDescriptor â†’ Structured Motion Data â”‚
â”‚  â€¢ Handshape classification                â”‚
â”‚  â€¢ Location tracking                       â”‚
â”‚  â€¢ Velocity & trajectory analysis          â”‚
â”‚  â€¢ Primitive detection                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       â”‚        â”‚        â”‚        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Cursor    â”‚ â”‚ JSON   â”‚ â”‚ Visual â”‚ â”‚ Futureâ”‚
â”‚ Control   â”‚ â”‚ Export â”‚ â”‚ Analysisâ”‚ â”‚Animationâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Key Features

### ğŸ¥ Real-Time Motion Capture

- 21-point hand tracking at ~30 FPS (MediaPipe)
- Coordinate normalization for cross-device consistency
- Motion smoothing and occlusion handling

### ğŸ§  Motion Descriptor Abstraction

- Converts landmarks â†’ structured linguistic motion data
- Classifies gesture primitives (POINT, FIST, OPEN_HAND, etc.)
- Tracks temporal sequences, velocity, and transitions

### ğŸ’¾ Data Pipeline

- JSON export for ML training and animation systems
- Metadata + timestamps + quality metrics
- Batch recording and dataset validation tools

### ğŸ“Š Analysis & Visualization

- Trajectory plots
- Primitive timelines
- Velocity profiles
- Gesture comparison tools

### ğŸ¨ Professional UI

- Split-screen interface (video + analytics)
- Live skeleton overlay
- Recording controls and session statistics

---

## ğŸ”¬ Why This Matters for Sign Language

Sign languages are built on four core parameters:

| Parameter   | How This System Captures It              | Status         |
| ----------- | ---------------------------------------- | -------------- |
| Handshape   | Finger states, landmark relationships    | âœ… Implemented |
| Location    | Normalized 2D coordinates                | âœ… Implemented |
| Movement    | Velocity, trajectory, temporal sequences | âœ… Implemented |
| Orientation | Landmark directions                      | âš ï¸ Partial     |

**Planned extensions:**

- Facial expressions (non-manual markers) â†’ MediaPipe Face
- Body position context â†’ MediaPipe Pose
- Two-handed coordination

---

## ğŸš€ Quick Start

### ğŸ Python Version Requirement

**Python 3.11 is required.**  
(MediaPipe does not currently support Python 3.12+.)

```bash
py -3.11 --version
```

### Installation

```bash
git clone https://github.com/richiekaroki/AIVirtualMouse.git
cd AIVirtualMouse
py -3.11 -m venv venv
venv\Scripts\activate   # Windows
# or
source venv/bin/activate  # Mac/Linux
pip install -r requirements.txt
```

### â–¶ï¸ Usage

#### 1. Real-Time Capture (Enhanced UI)

```bash
python AiVirtualMouseProject_Enhanced.py
```

**Controls:**

- **R** â€“ Start recording
- **S** â€“ Stop and save
- **C** â€“ Cancel
- **Q** â€“ Quit

#### 2. Batch Dataset Recording

```bash
python batch_record.py
```

#### 3. Motion Analysis

```bash
python MotionAnalyzer.py motion_data/gesture.json
python MotionAnalyzer.py motion_data/gesture.json --plot trajectory
python MotionAnalyzer.py motion_data/gesture.json --output plots/
python MotionAnalyzer.py gesture1.json gesture2.json --compare
```

---

## ğŸ“ Project Structure

```
AIVirtualMouse/
â”œâ”€â”€ MotionDescriptor.py        # Core abstraction â­
â”œâ”€â”€ MotionAnalyzer.py          # Analysis toolkit â­
â”œâ”€â”€ AiVirtualMouseProject_Enhanced.py
â”œâ”€â”€ batch_record.py
â”œâ”€â”€ analyze_dataset.py
â”œâ”€â”€ motion_data/
â”‚   â”œâ”€â”€ *.json
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ analysis_plots/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ SIGNLANGUAGE.md
â”‚   â”œâ”€â”€ ANALYSIS.md
â”‚   â”œâ”€â”€ CHANGELOG.md
â”‚   â””â”€â”€ recording_plan.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ dataset_summary.md
```

---

## ğŸ“Š Dataset Overview

- **45 total recordings** (15 gestures Ã— 3 attempts)
- **Average FPS:** 30
- **Average duration:** 2.5 seconds
- **Quality score:** 0.87 / 1.00

**Gesture categories include:**

- Static handshapes (fist, open hand, point, thumbs up)
- Dynamic movements (wave, circle, swipe)
- Directional motions (push, pull, point up)
- Transitions and complex gestures

---

## ğŸ“ Key Learnings

### Co-articulation is the hardest problem

Signs blend into each other â€” context matters.

### Sequences matter more than frames

A sign is a motion pattern, not a pose.

### Abstraction enables scalability

MotionDescriptor unlocked multiple downstream uses.

### Velocity and timing carry meaning

Motion dynamics affect semantics, not just appearance.

---

## ğŸ”® Roadmap

### Phase 1 â€” Extended Motion Capture

- MediaPipe Pose (body tracking)
- MediaPipe Face Mesh (expressions)
- Two-handed coordination
- 3D orientation extraction

### Phase 2 â€” Linguistic Layer

- Gloss annotation tools
- Co-articulation modeling
- Deaf community validation framework

### Phase 3 â€” Animation Output

- 3D rig integration (Blender / Three.js)
- Motion retargeting
- Keyframe generation

### Phase 4 â€” Translation Pipeline

- Text â†’ gloss (NLP)
- Gloss â†’ motion synthesis
- Real-time deployment

---

## ğŸ¤ Contributing

Contributions, suggestions, and feedback are welcome â€” especially in:

- Sign language expertise (especially Kenyan Sign Language)
- 3D animation and rigging
- ML/NLP translation systems
- Accessibility research

---

## ğŸ‘¤ Author

**Richard Kabue Karoki**  
ğŸ“ Nairobi, Kenya  
ğŸ“§ <karokirichard522@gmail.com>

ğŸŒ [GitHub](https://github.com/richiekaroki)  
ğŸ”— [LinkedIn](https://linkedin.com/in/richard-karoki-007)

**Education:** B.Sc. in Computer Technology, JKUAT (2024)  
**Experience:** 4+ years in backend/systems engineering, computer vision, and accessibility tech

---

## ğŸŒŸ Project Status

- **Status:** Active Development
- **Version:** 0.6.0
- **Last Updated:** January 2026
- **Next Milestone:** Animation integration

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Built with â¤ï¸ for accessibility and linguistic preservation**

â­ Star this repo if you find it useful!

</div>
